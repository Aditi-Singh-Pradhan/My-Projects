# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kznJNQ-DQQ9rLuqggCEYhL_JdCikgStK
"""

import numpy as np     #import libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

# update the path where CSV is stored in Drive
data = pd.read_csv('/content/drive/MyDrive/housing.csv')

data = pd.read_csv('/content/drive/MyDrive/housing.csv')

import os

for root, dirs, files in os.walk("/content/drive/MyDrive"):
    if "housing.csv" in files:
        print("Found at:", os.path.join(root, "housing.csv"))
        break

print(data.shape)       # rows, columns
print(data.columns)     # column names
data.head()             # first 5 rows
data.info()             # column types and non-null counts
data.describe()         # basic stats for numeric columns

data.isnull().sum()    #it counts missing values in each column. If any are non-zero, need to handle them (fill with median)

#this replaces all NaN values in total_bedrooms with the median of that column
#keeping the dataset consistent without dropping rows
median_value = data['total_bedrooms'].median()
data['total_bedrooms'].fillna(median_value, inplace=True)

#split dataset into features (X) and target (y)
#Here, the target is median_house_value, and the rest (except ocean_proximity which is categorical) are features

# target column
y = data['median_house_value'].values   #.values->converts column from a pandas Series into a plain NumPy array (so later we can use it in NumPy calculations)


# drop target + categorical column from features
X = data.drop(['median_house_value', 'ocean_proximity'], axis=1).values  #drop removes columns or rows,
                                                                         #axis=1 means “drop along the column axis” (if it were axis=0, it would drop rows)


print("Features shape:", X.shape)
print("Target shape:", y.shape)

"""The goal of that block of code is to Split your dataset into two parts:

Training set : the model will learn patterns from this data (about 80% of the rows).

Testing set : we keep this hidden until the end, so we can check how well the model performs on unseen data (about 20% of the rows).
"""

np.random.seed(42)      #locks the random generator to always start at the same point

n = X.shape[0]          #no of rows(no of houses)

indices = np.random.permutation(n)   #creates a shuffled list of row indices

split = int(n*0.8)     #80%train+20%test

train_idx, test_idx = indices[:split], indices[split:] # :split -> everything before split(excluding split)
                                                       # split: -> split pt + everything after

X_train, X_test = X[train_idx], X[test_idx]            #extracts the rows from X and y to create new arrays.
y_train, y_test = y[train_idx], y[test_idx]

print("Train shapes:", X_train.shape, y_train.shape)
print("Test shapes:", X_test.shape, y_test.shape)

"""The next step is **normalizing the features**.
Why do we normalize features?

feature columns(X) have very different scales.

If we run gradient descent like this, features with large values will dominate the updates.

Gradient descent will zig-zag and converge very slowly(or not at all).

Normalization fixes this:

We rescale each feature so it has mean = 0 and standard deviation = 1 (z-score normalization).

After this, all features are on the same scale, making training much smoother.
"""

#The plan is:
#Compute the mean and standard deviation of each feature column from the training set.
#Use those values to rescale both X_train and X_test.
#(Important: we only use training stats, so we don’t “peek” at the test set.)


feat_mean = X_train.mean(axis=0)   # mean of each column
feat_std = X_train.mean(axis=0)    #std of each column

feat_std[feat_std == 0] = 1       #boolean indexing, replaces any value of 0 in array with 1
                                  #to avoid divide by 0 if column has no variation


X_train_norm = (X_train - feat_mean) / feat_std   #normalize training and test sets using training stats
X_test_norm = (X_test - feat_mean) / feat_std



print("Before normalization:", X_train[0])     #first training row before normalization.
print("After normalization :", X_train_norm[0])  #the exact same row after normalization.

"""Imagine this

You have a cookie shop.

*   The more sugar you put in → the sweeter the cookie.
*   The more chocolate chips you add → the tastier it gets



But if someone asks:
“How tasty will this cookie be if I add 5 spoons of sugar and 10 chips?”

You want a formula that guesses tastiness from sugar and chips.

We need to find a formula like:

tastiness = (something × sugar) + (something × chips) + starting_point

How do we learn these “somethings”?

We guess them at first, test how close our prediction is to the real tastiness, then adjust:

If we guessed too low → increase the weight.

If we guessed too high → decrease the weight.

We keep adjusting again and again until our line predicts tastiness pretty well.

This “keep adjusting” is called Gradient Descent → just like slowly walking downhill until you reach the lowest point (the smallest error)
"""

class NumPyLinearRegression:                                               #defined a class
    def __init__(self, lr=0.01, n_iters=1000, verbose=False):              # def->defines a function, self->refers to the object itself, lr->learning rate, n_inters->default training steps, verbose=false->optional flag to print progress
                                                                           # used to set up model settings


        self.lr = lr                          #store the lr of the object
        self.n_iters = n_iters                #store the no of steps
        self.verbose = verbose                #store the flag
        self.w = None                         #weights will be created later
        self.loss_history = []                #empty list to save errors during training




    def _add_bias(self, X):                      #helper function to find bias term(intercept)
        ones = np.ones((X.shape[0], 1))          #make a column of 1 (with no of rows)
        return np.hstack([ones, X])              #np.hstack means “horizontal stack” → put arrays side by side (columns).
                                                 #[ones, X] is a list of arrays we want to stack.
                                                 #So this attaches the column of ones in front of X.



    def fit(self, X, y):                         # fit is a std ML name for "Train the Model", loop to adjust weight until error is small
      Xb = self._add_bias(X)                     # Xb-> same as X but with bias column
      n_samples, n_features = Xb.shape

      self.w = np.zeros((n_features, 1))        #creates a column vector of zeroes for weights, 1 per feature(+bias)
      y = y.reshape(-1, 1)                      #force y to be a column vector, -1 ->figure out the dimension, 1-> make it into a column


      for i in range(self.n_iters):             #run loop n_iters tiems

        pred = Xb.dot(self.w)                   #matrix multiplication ->predicted values

        error = pred - y                        #difference btw predictions and actual

        loss = np.mean(error ** 2) / 2          #compute mean squared error
        self.loss_history.append(loss)          #save it in loss_histroy so we can plot it later

        grad = (2/n_samples) * Xb.T.dot(error)  #slope of error

        self.w = self.w - self.lr * grad        #adjust the weights

        if self.verbose and i % 100 == 0:                    #every 100 steps,print current iteration and lose(if verbose=true)
                print(f"Iteration {i}, Loss: {loss:.4f}")    #f"...." -> f-string: lets u put variables directly inside a string


    def predict(self, X):                     #use trained weights to get new values
        Xb = self._add_bias(X)            #add bias to X
        return Xb.dot(self.w)             #multiplies by weights to get prediction

"""First guess: completely off.

They tell you “you guessed too low.”

Next time you guess higher.

Keep adjusting until your guesses match reality.

Training = error goes down, weights get better, model learns patterns.
"""

# Create and train model on normalized data
model = NumPyLinearRegression(lr=0.005, n_iters=5000, verbose=True)
model.fit(X_train_norm, y_train_norm)
# plot loss
plt.plot(model.loss_history)
plt.xlabel("Iteration")
plt.ylabel("MSE (normalized y)")
plt.title("Training loss")
plt.show()

# Predict (normalized), then rescale to original house values
y_train_pred_norm = model.predict(X_train_norm)   # shape (n,1)
y_test_pred_norm  = model.predict(X_test_norm)
y_train_pred = (y_train_pred_norm * y_std) + y_mean
y_test_pred  = (y_test_pred_norm  * y_std) + y_mean
# flatten for convenience
y_train_pred = y_train_pred.flatten()
y_test_pred  = y_test_pred.flatten()

"""#Evaluation Metrics for regression models

MSE-> Mean square root error, average of squared errors

RMSE -> Root mean square error, root of MSE

MAE-> Mean absolute error, average mistake value

R^2 -> Coefficient of Determination,

R^2 = 1 : Perfect prediction

......= 0 : no better than predicting mean

......<0 : worse than predicting the mean
  
"""

def mse(y_true, y_pred):
  return np.mean((y_true.flatten() - y_pred.flatten())**2)

def rmse(y_true, y_pred):
  return np.sqrt(mse(y_true, y_pred))

def mae(y_true, y_pred):
  return np.mean(np.abs(y_true.flatten() - y_pred.flatten()))

def r2_score(y_true, y_pred):
    ss_res = np.sum((y_true.flatten() - y_pred.flatten())**2)
    ss_tot = np.sum((y_true.flatten() - np.mean(y_true))**2)
    return 1 - ss_res/ss_tot


print("TRAIN Metrics")
print("MSE :", mse(y_train, y_train_pred))
print("RMSE:", rmse(y_train, y_train_pred))
print("MAE :", mae(y_train, y_train_pred))
print("R²  :", r2_score(y_train, y_train_pred))

print("\nTEST Metrics")
print("MSE :", mse(y_test, y_test_pred))
print("RMSE:", rmse(y_test, y_test_pred))
print("MAE :", mae(y_test, y_test_pred))
print("R²  :", r2_score(y_test, y_test_pred))

plt.figure(figsize=(10,4))

# scatter actual vs predicted
plt.subplot(1,2,1)
plt.scatter(y_test, y_test_pred, alpha=0.3, s=10)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Predicted vs Actual (test)")

# residuals histogram
plt.subplot(1,2,2)
residuals = y_test - y_test_pred
plt.hist(residuals, bins=50)
plt.title("Residuals (test)")
plt.xlabel("Error (actual - predicted)")
plt.tight_layout()
plt.show()